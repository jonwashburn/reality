% LaTeX document for Information-Limited Gravity Paper I
% File: Gravity-derived.tex

\documentclass[usenatbib]{mnras}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{array,tabularx}
\usepackage{listings}
\lstset{basicstyle=\ttfamily\small,breaklines=true,columns=fullflexible}
\citestyle{authoryear}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\makeatletter
\NAT@numberstrue
\makeatother

\title[Information-Limited Phenomenology of Disk Dynamics]{Paper I: Information-Limited Phenomenology of Disk Dynamics}

\author[J. Washburn]{Jonathan Washburn\thanks{E-mail: jon@recognitionphysics.org}\\
Recognition Physics Institute, Austin, TX, USA
}

\date{Submitted 2025 January}
\pubyear{2025}

\begin{document}

\maketitle

\begin{abstract}
We interpret gravity as the world's latency geometry and propose a falsifiable, global-only finite-refresh correction to the baryonic response in slow regimes. This work presents the conceptual foundations, information-theoretic motivation, and broader physical implications of the information-weight law $w(r)=\lambda\,\xi\,n(r)\,(T_{\text{dyn}}/\tau_\star)^\alpha\,\zeta(r)$. Quantitative validation on galaxy rotation curves is presented in the coordinated companion manuscript (Paper II).
\vspace{0.5em}
\noindent\textit{Series note.} This manuscript is part of a coordinated companion pair. \textbf{Paper I} (this work) defines the fixed, global phenomenology and its scope; \textbf{Paper II} tests that $w(r)$ on SPARC under identical masks/error model shared with all baselines. Both are co-submitted with shared artifacts and a single cover letter.
\end{abstract}

\begin{keywords}
gravitation -- galaxies: kinematics and dynamics -- dark matter -- methods: data analysis -- methods: numerical
\end{keywords}

\section{Introduction}
\paragraph*{Audit and freeze policy (summary).}
Late-time phenomenology uses the global anchor $\tau_\star$; Planck-gate quantities $\tau_0$ and $\lambda_{\rm rec}$ are confined to microscopic derivations. All displays respect the units quotient and the K-gate single-inequality audit. A global-only policy is enforced: single stellar $M/L$, identical masks/error model across model families, preregistered thresholds/profiles, and frozen commits for data and code.

For over three centuries, gravity has stood as physics' most familiar yet mysterious force. Newton provided the mathematical description, Einstein revealed the geometric nature, but neither explained why mass warps spacetime or attracts other mass. The discovery of galactic rotation anomalies \citep{Rubin1970} and cosmic acceleration \citep{Riess1998} has only deepened the mystery, spawning exotic solutions like dark matter particles and dark energy fields that together comprise 95 per cent of the universe yet remain undetected.

The dark matter paradigm, despite decades of searches, has yielded no direct detection. Experiments spanning 90 orders of magnitude in mass---from ultralight axions to primordial black holes---have found nothing. The parameter space for WIMPs, once the leading candidate, shrinks with each null result from ever-more-sensitive detectors. Meanwhile, simulations predict far more satellite galaxies than observed (the missing satellites problem), cuspy dark matter profiles that observations reject (the core-cusp problem), and struggle to explain the observed diversity of rotation curves (the diversity problem).

Modified gravity theories like MOND \citep{Milgrom1983} fare better empirically, reproducing galactic dynamics with remarkable economy. Yet MOND itself poses deep puzzles. Why should nature care about a particular acceleration scale $a_0 \approx 10^{-10}$\,m\,s$^{-2}$? How can a modification designed for galaxies also predict aspects of cosmology? Most troublingly, MOND's empirical success lacks a compelling theoretical foundation---it works too well to be wrong, yet no one knows why it works at all.

This work explores an alternative perspective inspired by information-theoretic approaches to physics \citep{Wheeler1990, Lloyd2002}. Building on ideas from holographic principles \citep{tHooft1993, susskind1995} and entropic gravity \citep{Verlinde2011, Jacobson1995}, we propose that gravitational phenomena emerge from constraints on information processing in a fundamental substrate. Unlike Verlinde's entropic gravity, which derives forces from thermodynamic gradients on holographic screens, or Jacobson's thermodynamic derivation of Einstein equations, our framework focuses on dynamic bandwidth limitations in field updates, leading to a distinct, optimization-based derivation of the MOND scale and galactic dynamics without additional fields or entropy assumptions.

Consider the computational challenge gravity presents. With $\sim 10^{80}$ particles in the observable universe, maintaining gravitational interactions requires processing an astronomical amount of information. Every mass must know about every other mass, fields must update as objects move, and all this must happen consistently across scales from subatomic to cosmic. No finite system could manage this exactly.

In this paper, we interpret gravity as the large-scale order of causal geometry and propose a falsifiable, global-only finite-refresh phenomenology for disk dynamics under information-theoretic constraints. Just as a computer operating system must allocate limited CPU cycles among competing processes, the substrate maintaining gravitational fields must manage finite bandwidth.

This bandwidth limitation motivates a specific, testable phenomenology for slow, extended systems: systems requiring frequent updates (like solar systems) consume more bandwidth and thus receive priority; slowly evolving disks can tolerate delayed updates. The resulting finite-refresh effect is encoded by a global, data-frozen multiplicative weight $w(r)$ applied to the baryonic response.

The paper now proceeds as follows: Section 2 revisits the informational foundations; Section 3 specifies the global-only phenomenology and its constraints; Section 4 connects timescale intuition to galactic dynamics; Section 5 outlines prospective relativistic directions; Section 6 summarizes how Paper II tests the same $w(r)$ under a strict global-only policy.

\section{What Gravity Is (and Why It Matters Here)}
\label{sec:what-gravity-is}

Gravity is not a bolt–on “extra force” in nature. It is the large–scale order of \emph{causal geometry}: the way spacetime sets which events can influence which, how long influence takes, and which paths are dynamically cheapest. In everyday language, gravity is the world’s \emph{latency structure}. Energy–momentum writes that structure; freely falling bodies read it by moving along the locally straight paths it defines. In fast, compact regimes this description reduces to the familiar Newtonian/relativistic limits; no discretionary parameters are introduced there.

\vspace{0.5em}
\noindent\textbf{Why gravity exists in a computational reality.}
If reality is a computation that must remain consistent at planetary, galactic, and cosmological scales \emph{without} a central referee, it needs:
(i) a strict causal order (no illegal shortcuts),
(ii) local latencies that reflect load (no global clock),
and (iii) self–assembled, long–lived arenas (stars and planets) where extended “programs” like chemistry, life, and society can run.
A geometric latency field does all three at once. That field is what we call gravity.
Time orders state updates; gravity \emph{distributes} that ordering across space and ties it to what is actually happening.

\vspace{0.5em}
\noindent\textbf{Where information limits enter (scope of this work).}
Nothing in the paragraph above redefines gravity. The present papers adopt a minimal, falsifiable \emph{phenomenology} for how \emph{finite information–refresh} interacts with ordinary baryonic geometry in extended, slowly evolving systems (galactic disks). The phenomenology is encoded by a dimensionless weight \(w(r)\) that multiplies the baryonic response,
\[
v_{\rm model}^2(r) \;=\; w(r)\,v_{\rm baryon}^2(r),
\qquad
a_{\rm eff}(r) \;=\; w(r)\,a_{\rm baryon}(r),
\]
with $w(r)\to 1$ wherever the local dynamics are fast (short $T_{\rm dyn}$) and measurement–like updates are effectively continuous, and $w(r)>1$ in slow, low–urgency environments where refresh lag is unavoidable. In this implementation this lag scales as a fixed power of the dynamical time,
\[
w(r)\;\propto\;\Big(\tfrac{T_{\rm dyn}(r)}{\tau_\star}\Big)^{\alpha},
\qquad \alpha \text{ fixed, global},
\]
modulated by global geometry factors $n(r)$ (radial anisotropy), $\zeta(r)$ (thickness/warp), and a frozen complexity proxy $\xi$.
Crucially, these are \emph{global–only} choices fixed once for the catalog, computed from photometric geometry and baryon maps—no per–galaxy tuning and no target leakage from the rotation curves themselves.

\vspace{0.5em}
\noindent\textbf{Interpretation: what the weight means and does not mean.}
The weight $w(r)$ is not a hidden halo, not a free interpolation function, and not a claim that “$\!G\!$” changes inside a galaxy. It is a compact, testable encoding of the single physical idea your analysis explores:
extended systems with long dynamical times must triage finite refresh, and that triage shows up as a small, predictable departure from the idealized, always–fresh baryonic response.
The resulting behavior naturally amplifies apparent gravity in dwarfs (large $T_{\rm dyn}$), leaves high–acceleration inner disks unchanged ($w\to 1$), and preserves the tight empirical relations (BTFR, RAR) without granting per–galaxy freedom.

\vspace{0.5em}
\noindent\textbf{Why galaxies look ``dark'' in this view.}
Galactic outskirts are slow theaters: long orbital periods, low accelerations, and thin signal–to–signal coupling. Under finite refresh, small lags accumulate coherently there, appearing observationally as an effective boost $w(r)>1$ to the baryonic curve. In inner disks, where clocks are fast and coupling is strong, the same mechanism collapses to unity and ordinary gravity is recovered. No extra knobs are introduced to make this happen; the single story is the scaling with $T_{\rm dyn}$ under globally fixed geometry.

\vspace{0.5em}
\noindent\textbf{Relation to time (and why this section belongs before methods).}
Time is the precondition for any computation at all; gravity is the spatial deployment of that computation’s timing constraints. The present phenomenology asks a focused, falsifiable question: \emph{Can strictly global, information–limited refresh, measured only from baryon geometry, account for the rotation–curve systematics we see?}
Everything else in the papers—global masks, floors, frozen thresholds, and baseline fairness—exists to make that single question auditable.

\vspace{0.5em}
\noindent\textbf{Scope and limits (what is \emph{not} claimed here).}
This section makes no relativistic or lensing claims and does not appeal to any exotic matter. The non–relativistic $w(r)$ phenomenology is presented as an empirical probe of finite–refresh effects in disks. A consistent relativistic completion, and any microphysical consequences it implies, must be tested separately against deflection, timing, and cosmology; until then, galaxy–scale fits should be read precisely as they are posed: a global–only, geometry–driven check of the finite–refresh hypothesis.

\vspace{0.5em}
\noindent\textbf{Practical reading guide for the rest of the paper.}
When reading the methods and results, keep three invariants in view:
(1) \emph{Object–level only}—all inputs to $w(r)$ come from photometric geometry and baryon fields; target velocities never inform the predictor;
(2) \emph{Global–only policy}—no per–galaxy parameters are adjusted; a single configuration governs the entire catalog;
(3) \emph{Fairness and falsifiability}—baselines see the same masks, floors, and data vectors; ablations and negative controls must break the effect if it is spurious.
Under those constraints, any improvement or failure is meaningful rather than a by–product of hidden flexibility.

\vspace{0.5em}
\noindent\textbf{Why this framing strengthens the papers.}
It clarifies that (i) gravity itself remains the causal geometry everyone agrees on; (ii) the novelty under test is \emph{where} small, lawful departures from “always–fresh” dynamics are allowed to appear and \emph{how} they are computed; and (iii) every modeling choice is global, frozen, and therefore auditable.
This keeps the result interpretable regardless of one’s priors about halos or modified dynamics: if the weight $w(r)$ succeeds under these constraints, a finite–refresh correction is empirically demanded; if it fails, the hypothesis is cleanly refuted.

\section{Foundational Premises}
\label{sec:foundations}

\subsection{Reality as Information Processing}

Following Wheeler's "it from bit" and recent developments in quantum information theory, we begin with the premise that reality fundamentally consists of information processing rather than material substance. This is not merely philosophical speculation---the holographic principle, black hole thermodynamics, and quantum error correction in AdS/CFT all point toward information as the fundamental currency of physics.

Key principle: Physical laws emerge from optimal information processing under constraints.

\subsection{The Substrate and Its Constraints}

Any system processing information faces three universal constraints that shape its behavior. First, finite bandwidth limits information transmission according to channel capacity, as formalized by the Shannon-Hartley theorem. Second, finite memory means that state storage requires physical resources, whether quantum states, classical bits, or more exotic representations. Third, optimization pressure ensures that limited resources must be allocated efficiently to maximize global utility.

We remain agnostic about the nature of this information-processing substrate, which may emerge from holographic degrees of freedom at spacetime boundaries \citep{susskind1995} or quantum computational limits \citep{lloyd2002}. The key insight is that regardless of its ultimate nature, any such substrate faces these constraints when maintaining gravitational fields across the universe.

The constraints become particularly severe when we consider the scale of the gravitational computation. With approximately $10^{80}$ particles in the observable universe, each potentially interacting with every other, the information processing requirements are staggering. Even restricting to gravitationally significant masses leaves an overwhelming computational burden that any finite system must manage through intelligent resource allocation.

\subsection{The Bandwidth Bottleneck}

Consider the computational demands of gravity. Every mass must interact with every other mass, leading to $N^2$ scaling in computational complexity. Fields must continuously update as objects move through space, maintaining consistency across all scales from subatomic to cosmic. Furthermore, information cannot propagate faster than light, imposing fundamental limits on update synchronization.

For a universe with $\sim 10^{80}$ particles, maintaining exact Newtonian gravity would require $\sim 10^{160}$ pairwise force calculations per update cycle. This is computationally prohibitive for any finite system.

\subsection{The Triage Solution}

Faced with overwhelming computational demands, any intelligent system would implement triage---prioritizing urgent updates while delaying less critical ones. We propose this is exactly what occurs in nature.

Solar systems receive the highest priority for updates due to their orbital periods ranging from days to years. The risk of collisions and complex N-body dynamics demand frequent attention. These systems update every fundamental update cycle, preserving Newtonian gravity to high precision.

Galaxy disks occupy a medium priority tier. With long rotation periods and quasi-circular orbits, they can tolerate less frequent updates. This triage picture motivates a finite-refresh correction in slow regimes (tested empirically in Paper II). A prospective cosmological extension is deferred to Outlook.

\section{Optimization-Based Motivation and Scaling}
\label{sec:derivation}

\subsection{Information Content of Gravitational Fields (heuristic)}

The gravitational field configuration for $N$ masses requires specifying complete information about the field at every point in space. This includes the field vector at each spatial location, comprising three directional components multiplied by the spatial resolution of our discretization. The field must be specified with sufficient precision to distinguish physically relevant differences in gravitational strength. Additionally, temporal consistency must be maintained across update cycles to ensure conservation laws remain satisfied.

As an order-of-magnitude heuristic, several arguments (e.g., holographic considerations \citep{Bekenstein1973, tHooft1993}) suggest that field upkeep scales steeply with system size and resolution. A schematic bound reads:
\begin{equation}
I_{\text{field}} \leq 3 \times \left(\frac{L^2}{\ell_{\text{min}}^2}\right) \times \log_2\left(\frac{g_{\text{max}}}{g_{\text{min}}}\right) \times N_{\text{interactions}}
\end{equation}

This expression is illustrative only; precise coefficients are not needed in this paper. For a typical galaxy with characteristic size $L \sim 100$\,kpc and minimum resolution $\ell_{\text{min}} \sim 1$\,pc, such counts are large enough to require triage in any finite-refresh setting.

\subsection{Channel Capacity Constraints}

The total information flow for gravitational updates cannot exceed channel capacity:
\begin{equation}
\sum_{\text{systems}} \frac{I_{\text{system}}}{\Delta t_{\text{system}}} \leq B_{\text{total}}
\end{equation}
where $B_{\text{total}}$ is the total available bandwidth and $\Delta t_{\text{system}}$ is the refresh interval for each system.

\subsection{Optimization Problem}

The substrate must solve:
\begin{align}
\text{maximize: } & \sum_i U_i(\Delta t_i) \quad \text{[total utility]} \
\text{subject to: }\\ & \sum_i \frac{I_i}{\Delta t_i} \leq B_{\text{total}} \quad \text{[bandwidth constraint]}
\end{align}
where $U_i$ represents the "utility" of updating system $i$ frequently.

Natural utility function: $U_i = -K_i \times \Delta t_i^\alpha$ where $K_i$ is the urgency factor (collision risk, dynamical complexity), $\alpha$ is the diminishing returns exponent, and the negative sign ensures longer delays reduce utility.

\subsection{Utility Function Selection}

What utility function should the substrate use? Consider physical requirements. Shorter delays are always preferred: $dU/d\Delta t < 0$. Diminishing returns apply: $d^2U/d\Delta t^2 < 0$. Scale invariance requires: $U(k\Delta t) = k^\alpha U(\Delta t)$.

These constraints suggest:
\begin{equation}
U_i(\Delta t_i) = -K_i \Delta t_i^\alpha
\end{equation}
where $K_i$ represents the "urgency" of system $i$.

To see why this form emerges naturally, consider the general scale-invariant utility satisfying our constraints. Any such function must satisfy the functional equation:
\begin{equation}
U(\lambda \Delta t) = f(\lambda) U(\Delta t)
\end{equation}
for some function $f$. Taking derivatives with respect to $\lambda$ and setting $\lambda = 1$ yields:
\begin{equation}
\Delta t U'(\Delta t) = f'(1) U(\Delta t)
\end{equation}
This differential equation has the general solution $U(\Delta t) = C \Delta t^{\alpha}$ where $\alpha = f'(1)$. The requirement that utility decreases with delay ($dU/d\Delta t < 0$) implies $\alpha > 0$ and $C < 0$, giving our form with $C = -K_i$.

We adopt a single global exponent $\alpha=0.191$ (not fitted per galaxy), consistent with the RS-motivated choice used in Paper II. Sensitivity bands are reported there; convexity requires $\alpha<2$.

Physical factors affecting urgency include collision risk for systems with crossing orbits, dynamical complexity from N-body chaos and resonances, observable importance for systems hosting observers, and energy density where high-energy regions need accuracy.

\subsection{Lagrangian Solution}

Using Lagrange multipliers:
\begin{equation}
\mathcal{L} = \sum_i (-K_i \Delta t_i^\alpha) - \mu\left(\sum_i \frac{I_i}{\Delta t_i} - B_{\text{total}}\right)
\end{equation}

Taking derivatives:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \Delta t_i} = -\alpha K_i \Delta t_i^{\alpha-1} + \mu \frac{I_i}{\Delta t_i^2} = 0
\end{equation}

Solving for optimal refresh interval:
\begin{equation}
\Delta t_i^* = \left(\frac{\mu I_i}{\alpha K_i}\right)^{1/(2-\alpha)}
\end{equation}

This reveals the key scaling: systems with more information content $I_i$ receive LONGER refresh intervals, while urgent systems (high $K_i$) receive SHORTER intervals.

\subsection{Recognition Weight Function}

The refresh lag creates a mismatch between the actual field and ideal Newtonian field. We define the information weight as:
\begin{equation}
w = \frac{\text{effective gravity}}{\text{Newtonian gravity}}
\end{equation}

During the interval $\Delta t$ between updates, objects continue moving while fields remain static. For circular orbits, this creates an effective boost (illustrative small-lag form):
\begin{equation}
w \approx 1 + \frac{v \Delta t}{r} \approx 1 + \frac{\Delta t}{T_{\text{dyn}}}
\end{equation}
where $T_{\text{dyn}} = 2\pi r/v$ is the dynamical time. In the regime we fit, we assume a global lag-scaling $\Delta t = C\,T_{\rm dyn}^{\alpha}$ (single global constant $C$), giving at first order
\begin{equation}
w \approx 1 + C\, T_{\rm dyn}^{\alpha-1}.
\end{equation}
The production kernel used in Paper II folds $C$ (and any reference units) into a single global normalization, yielding the power-law form stated there. This makes the small-lag series and the production kernel consistent at leading order.

To understand this physically, consider a star orbiting in a galaxy. At time $t_0$, the gravitational field is updated based on the mass distribution. The star experiences the correct force and begins its orbital motion. However, the field remains frozen until the next update at $t_0 + \Delta t$. During this interval, the star has moved to a new position, but continues experiencing the force from its original location. This mismatch between where the star is and where the field "thinks" it is creates an apparent extra force.

For slow-moving systems where $\Delta t \ll T_{\text{dyn}}$, this effect is negligible---the star barely moves between updates. But in galaxies where $\Delta t \sim T_{\text{dyn}}$, the star completes a significant fraction of its orbit between updates. The accumulated error manifests as additional centripetal acceleration, exactly mimicking the effect of extra unseen mass. This is why dark matter appears to trace visible matter so perfectly---it's not a coincidence but a direct consequence of refresh lag scaling with the visible mass distribution.

\subsection{Emergent Acceleration Scale}

The transition between Newtonian and finite–refresh–affected regimes occurs when refresh lag becomes significant:
\begin{equation}
\Delta t \sim T_{\text{dyn}}
\end{equation}

For galaxies with $\Delta t \sim 10^8$\,yr:
\begin{equation}
T_{\text{dyn}} \sim 10^8$\,yr $\rightarrow \frac{v^2}{r} \sim 10^{-10}$\,m\,s$^{-2}$
\end{equation}

This naturally produces the MOND acceleration scale $a_0$ without fine-tuning!

\subsection{Physical Interpretation of the Emergent Scale}

The emergence of a characteristic acceleration scale $a_0 \sim 10^{-10}$ m/s$^2$ from our bandwidth framework deserves deeper examination. This is an order-of-magnitude consistency observation (no tuning to $a_0$): long dynamical times naturally map to low accelerations.

In our framework, $a_0$ represents the acceleration at which refresh lag effects become comparable to the dynamical time. Below this acceleration, systems evolve so slowly that even infrequent updates suffice to maintain approximate Newtonian behavior. Above this acceleration, rapid dynamics demand frequent updates that the bandwidth-limited substrate can provide.

The numerical value of $a_0$ emerges from the intersection of several cosmic timescales. The age of the universe sets the overall temporal context. The fundamental update cycle time, derived from LNAL principles, determines the fundamental update frequency. The typical refresh interval for galactic systems, emerging from optimization under bandwidth constraints, provides the final ingredient. When these timescales combine, they naturally produce an acceleration scale matching observations.

This explains why $a_0$ appears universal despite arising from a complex optimization process. The bandwidth constraints and utility functions are themselves universal, leading to consistent resource allocation patterns across different systems. Just as the speed of light emerges as a universal limit from special relativity, $a_0$ emerges as a universal scale from bandwidth-limited gravity.

Furthermore, this interpretation makes testable predictions. Systems with unusual complexity or dynamics should show deviations from the standard $a_0$ value. Young galaxies at high redshift, with different evolutionary histories, might exhibit slightly different transition scales. These predictions distinguish our framework from MOND, where $a_0$ is simply postulated as fundamental.

\section{Complete Mathematical Formalism}
\label{sec:formalism}

\subsection{Information Weight Definition}

Combining all factors, the information weight becomes:
\begin{equation}
w(r) = \lambda \times \xi \times n(r) \times \left(\frac{T_{\text{dyn}}}{\tau_\star}\right)^\alpha \times \zeta(r)
\end{equation}

Each component serves a distinct physical purpose. The normalization $\lambda$ absorbs any reference-scale choice (including $\tau_\star$); we do not fit or interpret $\tau_\star$ directly. The complexity factor $\xi$ captures object-level variation via frozen catalog quantiles; the spatial profile $n(r)$ encodes a universal anisotropy with a disc-weighted normalization; the dynamical-time factor encodes slower refresh in slow regimes; and $\zeta(r)$ captures bounded thickness/warp effects. In production tests (Paper II) we use centered kernels with $C_{\rm lag}=\varphi^{-5}$, and (for the acceleration-form) a single baryon-derived $g_{\rm ref}$ fixed at the catalog level.

\paragraph{Global constants (catalog-global; not per-galaxy).} We adopt a single global set shared across the catalog: $\alpha=0.191$; $C_{\rm lag}=\varphi^{-5}$; analytic $n(r)$ with $(A,r_0,p)=(7,8\,\mathrm{kpc},1.6)$ and disc-weighted normalization; bounded $\zeta$ with $h_z/R_d=0.25$; threads-informed $\xi$ with $B=5$ global bins and $C_\xi=\varphi^{-5}$; and a single global stellar $M/L$. The reference scale $\tau_\star$ is absorbed into $\lambda$. None of these are fitted per galaxy.

\paragraph{Assumptions summary.}
\begin{quote}
  (i) Late-time anchor $\tau_\star$ is held fixed across all galaxies; (ii) global constants $(\alpha, C_{\rm lag}, n(r), \zeta, \xi)$ are frozen prior to consuming rotation curves; (iii) photometric geometries determine all orientation inputs; (iv) identical mask/error policies apply to ILG, MOND, and $\Lambda$CDM baselines.
\end{quote}

\subsection{Complexity Factor}

Systems with complex dynamics require more frequent updates. We adopt a \emph{global-only, frozen-quantile} proxy that is computed once and then held fixed for all analyses:
\begin{equation}
\xi = 1 + C_\xi\, u_b^{1/2}, \qquad u_b = \frac{b+\tfrac{1}{2}}{B},\quad b\in\{0,\dots,B{-}1\}.
\end{equation}
Here $B$ is the number of global bins (default $B{=}5$), and each galaxy is assigned an index $b$ by applying predeclared quantile thresholds to a catalog \emph{object-level} complexity proxy (e.g., true gas fraction). These thresholds are computed on a small calibration subset and then \emph{frozen} under the analysis commit; they are not recomputed on the analysis sample. We use $C_\xi = \varphi^{-5} \approx 0.090$.

\subsection{Spatial Profile}

The function $n(r)$ captures how refresh priority varies within a galaxy. We adopt the fixed analytic form
\begin{equation}
n_{\rm analytic}(r) = 1 + A\left[1 - \exp\!\bigl(-(r/r_0)^p\bigr)\right],
\end{equation}
with default global parameters $(A, r_0, p) = (7, 8$\,kpc, $1.6)$, normalised so that a disc-weighted mean equals unity. A legacy cubic-spline path exists for diagnostics but is not used in production analyses.

\subsection{Dynamical Time Factor}

The dynamical time dependence emerges from the Lagrangian optimization:
\begin{equation}
\left(\frac{T_{\text{dyn}}}{\tau_\star}\right)^\alpha \quad \text{with} \quad T_{\text{dyn}} = \frac{2\pi r}{v_{\text{baryon}}}
\end{equation}

where $v_{\text{baryon}}^2 = v_{\text{gas}}^2 + v_{\text{disk}}^2 + v_{\text{bulge}}^2$ is constructed from photometric/baryonic components only (no target $v_{\text{obs}}$ anywhere in the predictor). Fixed value: $\alpha = 0.191$

The modest exponent indicates robust bandwidth allocation---not extreme triage but consistent prioritization.

\subsection{Modified Rotation Curve}

The observed rotation velocity becomes:
\begin{equation}
v_{\text{model}}^2(r) = w(r) \times v_{\text{baryon}}^2(r)
\end{equation}
where $v_{\text{baryon}}^2 = v_{\text{gas}}^2 + v_{\text{disk}}^2 + v_{\text{bulge}}^2$ is the Newtonian prediction.

This simple multiplication by $w(r)$ transforms failing Newtonian predictions into accurate fits.

\section{Empirical Validation}

Quantitative validation on rotation curves, masks/floors, ablations, and baselines is presented in the coordinated companion manuscript (Paper II). This paper confines itself to the definition, scope, and falsifiability of the fixed, global phenomenology.

\section{Qualitative Empirical Support}
\label{sec:qualitative}

A full statistical confrontation with rotation-curve data is presented in the companion Paper II under an identical global-only policy. Here we limit ourselves to noting that the information-weight law with globally fixed constants is designed to preserve high-acceleration limits while enhancing effective response in slow, low-urgency regimes.

Empirical validation on galaxy rotation curves appears in Paper II.

\section{Discussion}

\noindent\textit{Companion pointer.} Empirical validation of the fixed, global phenomenology defined here appears in \textbf{Paper II} under an identical global-only policy with shared masks, floors, ablations, and fair baselines.

\section{Comparison to Alternatives}

Quantitative comparisons with MOND and ΛCDM under shared masks/error-models are provided in the companion Paper II. The present paper focuses on the auditable, global-only phenomenology being tested.

\section{Implications and Outlook}
\label{sec:conclusion}

We have proposed a global-only, finite-refresh phenomenology for disk dynamics motivated by information constraints. If validated further, it could provide insights into dark-sector phenomenology while leaving GR/Newtonian dynamics intact in fast, compact regimes where $w\to 1$.

Predictions (prospective and to be tested with a relativistic completion): possible redshift-dependent transitions in slow-regime behaviour, subtle lensing signatures consistent with GR constraints, and small shifts in cosmological observables. No quantitative claims are made here.

Limitations: The substrate remains agnostic; any relativistic completion must satisfy Solar-System, binary-pulsar, and cosmological bounds. This paper intentionally avoids quantitative lensing/cosmology claims.

\appendix

\section{Detailed Information-Theoretic Derivation}

\subsection{Configuration Space Analysis}

For $N$ gravitating masses, the full configuration space has dimension $6N$ (positions and velocities). The gravitational field must encode sufficient information to determine forces on test particles anywhere in space.

Consider discretizing space into cells of size $\ell_{\text{min}}$. The number of cells is:
\begin{equation}
N_{\text{cells}} = \left(\frac{L}{\ell_{\text{min}}}\right)^3
\end{equation}

At each cell, we need the gravitational field vector (3 components), precision of $\log_2(g_{\text{max}}/g_{\text{min}})$ bits per component, giving total $I_{\text{cell}} = 3 \log_2(g_{\text{max}}/g_{\text{min}})$ bits.

Total field information:
\begin{equation}
I_{\text{field}} = N_{\text{cells}} \times I_{\text{cell}} = 3\left(\frac{L}{\ell_{\text{min}}}\right)^3 \log_2\left(\frac{g_{\text{max}}}{g_{\text{min}}}\right)
\end{equation}

\subsection{Update Frequency Optimization}

The substrate must decide how often to update each system's gravitational field. Define $\Delta t_i$ as the refresh interval for system $i$, $I_i$ as the information content of system $i$, and $B_i = I_i/\Delta t_i$ as the bandwidth consumed by system $i$.

Total bandwidth constraint:
\begin{equation}
\sum_i B_i = \sum_i \frac{I_i}{\Delta t_i} \leq B_{\text{total}}
\end{equation}

The optimization problem becomes:
\begin{align}
\text{maximize: } & U_{\text{total}} = \sum_i U_i(\Delta t_i) \
\text{subject to: }\\ & \sum_i \frac{I_i}{\Delta t_i} \leq B_{\text{total}}
\end{align}
where $U_i(\Delta t_i)$ represents the utility of updating system $i$ with interval $\Delta t_i$.

\subsection{Utility Function Selection}

What utility function should the substrate use? Consider physical requirements. Shorter delays are always preferred: $dU/d\Delta t < 0$. Diminishing returns apply: $d^2U/d\Delta t^2 < 0$. Scale invariance requires: $U(k\Delta t) = k^\alpha U(\Delta t)$.

These constraints suggest:
\begin{equation}
U_i(\Delta t_i) = -K_i \Delta t_i^\alpha
\end{equation}
where $K_i$ represents the "urgency" of system $i$.

To see why this form emerges naturally, consider the general scale-invariant utility satisfying our constraints. Any such function must satisfy the functional equation:
\begin{equation}
U(\lambda \Delta t) = f(\lambda) U(\Delta t)
\end{equation}
for some function $f$. Taking derivatives with respect to $\lambda$ and setting $\lambda = 1$ yields:
\begin{equation}
\Delta t U'(\Delta t) = f'(1) U(\Delta t)
\end{equation}
This differential equation has the general solution $U(\Delta t) = C \Delta t^{\alpha}$ where $\alpha = f'(1)$. The requirement that utility decreases with delay ($dU/d\Delta t < 0$) implies $\alpha > 0$ and $C < 0$, giving our form with $C = -K_i$.

The parameter $\alpha$ controls how steeply utility drops with delay. For $\alpha < 1$, utility decreases sublinearly---a system can tolerate delays with modest penalty. For $\alpha > 1$, delays become increasingly costly. The value $\alpha < 2$ ensures the optimization problem remains convex and has a unique solution.

Physical factors affecting urgency include collision risk for systems with crossing orbits, dynamical complexity from N-body chaos and resonances, observable importance for systems hosting observers, and energy density where high-energy regions need accuracy.

\subsection{Solving the Lagrange System Explicitly}

Let the global bandwidth be $B_{\text{total}}$ and define the Lagrange multiplier $\mu$ such that
\begin{equation}
\mu = \frac{\alpha K_i \Delta t_i^{\alpha+1}}{I_i}
\end{equation}

Combining this with the constraint $\sum_i I_i / \Delta t_i = B_{\text{total}}$ yields
\begin{equation}
\mu^{(2-\alpha)/(1+\alpha)} = \frac{\alpha^{(2-\alpha)/(1+\alpha)} \left( \sum_i I_i^{(1-\alpha)/(1+\alpha)} K_i^{(2-\alpha)/(1+\alpha)} \right)}{B_{\text{total}}^{(2-\alpha)/(1+\alpha)}}
\end{equation}

Substituting $\mu$ back, the optimal refresh interval for system $i$ becomes
\begin{equation}
\Delta t_i^* = C \left( \frac{I_i}{K_i} \right)^{1/(2-\alpha)}
\end{equation}
where 
\begin{equation}
\begin{split}
C = B_{\text{total}}^{1/(2-\alpha)} \Bigg[ \alpha^{-1/(2-\alpha)} & \\
\times \left(\sum_j I_j^{(1-\alpha)/(1+\alpha)} K_j^{(2-\alpha)/(1+\alpha)} \right)^{-1/(2-\alpha)} \Bigg]
\end{split}
\end{equation}

Hence the refresh interval scales as $\Delta t \propto I^{1/(2-\alpha)}$ for fixed urgency.

\subsection{Connecting Refresh Lag to Effective Force}

For small lag ($\Delta t \ll T_{\text{dyn}}$) the leading correction to the Newtonian potential $\Phi_N$ is second order in time. A star of speed $v$ moves a distance $v\Delta t$ between field evaluations. Expanding the Newtonian field to first order in this displacement produces an effective potential
\begin{equation}
\Phi_{\text{eff}} = \Phi_N + \frac{\Delta t}{T_{\text{dyn}}} \Phi_N + \mathcal{O}\left(\left(\frac{\Delta t}{T_{\text{dyn}}}\right)^2\right)
\end{equation}

so that the square-velocity relation becomes $v^2 = R \partial\Phi_{\text{eff}}/\partial R = w v_N^2$ with $w = 1 + \Delta t/T_{\text{dyn}}$.

\subsection{Outlook: Toward a Relativistic Extension}

In fast, compact regimes ($T_{\rm dyn}$ short) the weight tends to unity ($w\to 1$), leaving standard Solar-System and binary-pulsar tests intact. A full relativistic treatment would ultimately encode refresh effects in a consistent covariant completion and is left for future work.

\subsection{Computation of w(r)}

Implementation details and numerics are provided in the companion Paper II.

\section{Scope of Empirical Tests}
Empirical protocols (cross-validation, robustness, and residual diagnostics) are documented and executed in Paper II under a strict global-only policy; we do not include those results here.

\section*{Data Availability}

The data underlying this article and all analysis code are available at \href{https://github.com/jonwashburn/gravity}{https://github.com/jonwashburn/gravity}. An archival snapshot with artifacts is available at Zenodo (DOI: \href{https://doi.org/10.5281/zenodo.16014943}{10.5281/zenodo.16014943}).

\section*{Acknowledgments, Funding, and Competing Interests}

This work received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors. The author declares no competing interests.

\bibliographystyle{mnras}
\bibliography{references-ilg}

\end{document} 
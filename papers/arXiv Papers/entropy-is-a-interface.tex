\documentclass[11pt,letterpaper]{article}

% Packages
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{tcolorbox}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=NavyBlue,
    citecolor=BrickRed,
    urlcolor=MidnightBlue,
    pdfauthor={Jonathan Washburn},
    pdftitle={Entropy Is an Interface: Reversibility in the Substrate, Irreversibility at Commit}
}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{prediction}[theorem]{Prediction}
\newtheorem{falsifier}[theorem]{Falsifier}

% Custom boxes
\newtcolorbox{keypoint}{
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=Key Point
}

\newtcolorbox{workednumeric}{
    colback=green!5!white,
    colframe=green!50!black,
    fonttitle=\bfseries,
    title=Worked Numerical Example
}

% Headers
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small\itshape Entropy Is an Interface}
\fancyhead[R]{\small\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% Title formatting
\title{%
  \vspace{-1cm}%
  \Large\textbf{Entropy Is an Interface:}\\[0.3em]
  \large Reversibility in the Substrate, Irreversibility at Commit\\[1em]
  \normalsize Recognition Science Paper Series
}

\author{
  \textbf{Jonathan Washburn}\\
  Recognition Physics Institute\\
  Austin, Texas, USA\\
  \texttt{jon@recognitionphysics.org}\\[0.5em]
  \textsc{Preprint} --- \today
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
\noindent This paper reframes entropy as an \emph{interface quantity}: the minimal code length required to specify instrument--distinguishable outcomes under a declared measurement channel. A channel is given by a coarse--graining \emph{window} \(W\!:\!X\!\to\!Z\) and a noise/response \emph{kernel} \(K(y\mid z)\), which together induce an observable distribution \(p_Y(y)=\int K(y\mid W(x))\,d\mu(x)\) from a state distribution \(\mu\) on \(X\). The operational entropy is
\[
S_{W,K}(\mu)\;:=\;L^\ast(p_Y),
\]
the optimal prefix--free codelength (MDL/Shannon length in the ideal limit) for draws from \(p_Y\). With this definition, the second law is clarified: entropy does not increase during reversible micro--evolution that leaves \(p_Y\) unchanged; it \emph{does} increase at recognition \emph{commits} (writes/binnings/erasures) by the data--processing inequality, and the thermodynamic work cost of erasure obeys \(W_{\min}\ge k_BT\ln 2\cdot\Delta S\) with \(\Delta S\) measured in bits at the channel. This interface view recovers textbook entropies (Gibbs/Boltzmann/Shannon) as special cases, predicts how reported entropy depends lawfully on the chosen \((W,K)\), and explains Maxwell--demon accounting without paradox: irreversibility is the bill paid at commit, not a defect of the substrate. The paper develops a methods--first protocol: declare \((W,K)\), compute \(S_{W,K}\) as a codelength, and report entropy production as code--length increments across commits. Archival demonstrations (blackbody spectra, atomic line lists, and quasi--static gas processes) illustrate the accounting with no new experiments. The framework is falsifiable: any reproducible decrease of \(S_{W,K}\) across a commit without compensating exports of negentropy would refute it. Reproducibility is enforced via preregistered channels, fixed scoring rules, and one--command rebuilds of all numbers.
\end{abstract}

\vspace{1em}

\begin{keypoint}
\textbf{Core Claim:} Entropy lives at the measurement interface, not in the substrate. Reversible bulk dynamics conserve \(S_{W,K}\); commits (writes/erasures) increase it monotonically. This resolves the reversibility paradox and makes entropy accounting reproducible and falsifiable.
\end{keypoint}

\tableofcontents
\newpage

\section{Introduction}

\subsection{The Reversibility Paradox}

We teach entropy as ``disorder'' in matter, yet many physical systems are microscopically reversible: Hamiltonian evolution is one--to--one and conserves phase--space volume, quantum evolution is unitary. Irreversibility shows up only when we \emph{record}, \emph{bin}, and \emph{erase}---the practical steps by which measurements are turned into persistent symbols.

\subsection{Central Claim}

\textbf{Entropy lives at the \emph{interface}}---the measurement channel determined by a coarse--graining \emph{window} \(W\!:\!X\!\to\!Z\) and a noise/response \emph{kernel} \(K(y\!\mid\!z)\). Given a state distribution \(\mu\) on \(X\), the channel induces the observable distribution
\[
p_Y(y) \;=\; \int K\!\left(y \,\middle|\, W(x)\right)\, d\mu(x),
\]
and the operational entropy is the minimum codelength needed to specify draws from this \(p_Y\).

\subsection{Contributions}

This paper is methods--first and makes five concrete contributions:

\begin{enumerate}[leftmargin=*]
  \item \textbf{Operational definition.} We define
  \[
  S_{W,K}(\mu)\;:=\;L^\ast\!\big(p_Y\big),
  \]
  the optimal prefix--free codelength (MDL/Shannon length in the ideal limit) to encode readouts \(Y\) induced by the declared channel \((W,K)\).
  
  \item \textbf{Arrow as commit--monotone.} Entropy does not increase during reversible micro--steps that leave \(p_Y\) unchanged; it \emph{does} increase at \emph{commits} (writes/binnings/erasures) by the data--processing inequality. In short: irreversibility is a property of the recognition pipeline, not of the substrate dynamics.
  
  \item \textbf{Landauer mapping.} Work cost attaches to erasure/commit, not generic dynamics. Erasing \(\Delta S\) bits at temperature \(T\) requires at least \(k_B T\ln 2 \cdot \Delta S\); here \(\Delta S\) is measured directly as a code--length increment at the channel.
  
  \item \textbf{Archival demonstrations.} Using only public data (no new experiments), we show interface--dependent entropy budgets that match this accounting across: (i) calibrated blackbody spectra, (ii) atomic line lists (H/He), and (iii) quasi--static gas processes. Each case declares \((W,K)\), computes \(S_{W,K}\), and reports entropy production as codelength increments across commits.
  
  \item \textbf{Falsifiers and preregistered robustness.} We state concrete failure modes (e.g., a reproducible decrease of \(S_{W,K}\) across a commit without exporting negentropy) and preregister a robustness protocol (fixed channels, fixed scoring, dual--implementation codelength checks, and one--command rebuilds) so that all numbers are auditable.
\end{enumerate}

\noindent Throughout, a \emph{commit} means a step that writes a persistent symbol, bins outcomes, or erases a record, thereby fixing \((W,K)\) for that step and disallowing retroactive refinement. With this interface framing, ``reversibility below, irreversibility at commit'' becomes a measurable rule: entropy is the code we must pay to say what our instrument can in fact distinguish.

\subsection{Recognition Science Integration}

This work builds on Recognition Science (RS)~\cite{RS_Source}, a parameter-free framework where dynamics are generated by a Recognition Operator \(\widehat{R}\) that minimizes a unique convex symmetric cost \(J(x)=\tfrac{1}{2}(x+1/x)-1\) (T5: cost uniqueness) rather than energy. Key RS results used here:

\begin{itemize}[leftmargin=*]
  \item \textbf{Continuity (T3):} Closed-loop flux equals zero on the ledger substrate, guaranteeing microscopically reversible dynamics~\cite{RS_T3}.
  \item \textbf{Eight-tick periodicity (T6):} Minimal update period is \(2^D\) (for \(D=3\), eight ticks); aligned measurement windows preserving this periodicity conserve invariants~\cite{RS_T6}.
  \item \textbf{Pattern measurement lemmas:} For 8-periodic patterns, aligned block sums equal the Z-invariant; misalignment creates predictable alias entropy~\cite{RS_PatternMeasurement}.
\end{itemize}

\noindent These RS foundations anchor the claim that \emph{substrate reversibility} is exact (flux conservation) and that \emph{alignment with invariants} predicts when entropy is preserved versus inflated.

\section{Background and Related Work}

\subsection{Thermodynamic and Statistical Entropy}

Clausius framed entropy via \(dS = \delta Q_\mathrm{rev}/T\); Boltzmann and Gibbs linked entropy to state counting and ensembles, with coarse--graining introduced to connect microstates to macroscopic variables. Information--theoretic views identify entropy with code length (Shannon~\cite{Shannon1948}) and model selection penalties (MDL~\cite{Rissanen1978}), making explicit the dependence on how outcomes are represented and encoded.

\subsection{Landauer and Measurement}

Landauer's principle~\cite{Landauer1961} ties erasure of one bit at temperature \(T\) to a minimal work cost \(k_B T\ln 2\), relocating thermodynamic payment to memory operations. Maxwell--demon analyses that include the demon's record--keeping find no paradox: the demon pays in commits (writes/erases), not in the underlying reversible dynamics~\cite{Bennett1982}.

\subsection{Gap This Paper Closes}

We supply a single, testable \emph{interface} definition of entropy that (a) aligns directly with MDL as code length, (b) pins irreversibility to explicit commit steps in the recognition pipeline, and (c) predicts how reported entropy shifts when the instrument's window and kernel are changed in declared ways. The result is an operational recipe rather than a metaphor: declare the channel, compute code lengths, and audit entropy production as increments across commits.

\section{Definitions: The Interface View}

\begin{definition}[Measurement Channel]
Let \(X\) be the state space and \(Y\) the readout space. A \emph{window} \(W\!:\!X\!\to\!Z\) performs coarse--graining to a latent instrument space \(Z\), and a \emph{kernel} \(K(y\mid z)\) encodes the instrument's response/noise from \(Z\) to \(Y\). Given a state distribution \(\mu\) on \(X\), the observed distribution on \(Y\) is
\begin{equation}\label{eq:channel}
p_Y(y) \;=\; \int K\!\left(y \,\middle|\, W(x)\right)\, d\mu(x).
\end{equation}
\end{definition}

\begin{definition}[Operational Entropy]
Define the interface entropy as the minimal prefix--free codelength for draws from the observed distribution:
\begin{equation}\label{eq:entropy}
S_{W,K}(\mu) \;:=\; L^\ast\!\big(p_Y\big),
\end{equation}
which reduces to the Shannon length in the ideal coding limit and is computed via MDL in practice (model code + parameter code to supported precision + residual code under the declared noise model).
\end{definition}

\begin{definition}[Commit]\label{def:commit}
A \emph{commit} is a step that writes symbols to a persistent record, bins outcomes, or erases stored information, thereby fixing \((W,K)\) for that step and disallowing retroactive refinement. Entropy production is accounted as code--length increments across such commits; reversible micro--evolution between commits leaves \(p_Y\) (and hence \(S_{W,K}\)) unchanged.
\end{definition}

\section{Core Results (Method-Level Statements)}

\begin{theorem}[Monotonicity Under Post-Processing]\label{thm:monotonicity}
Let a commit apply a stochastic map \(Q\!:\!Y\!\to\!Z\) (further binning/coarse--grain), so that
\[
p_Z(z)\;=\;\sum_{y\in Y} Q(z\mid y)\,p_Y(y).
\]
Then the interface entropy is monotone up to a reference--machine constant:
\[
S_Z \;=\; L^\ast(p_Z)\;\ge\;L^\ast(p_Y)\;-\;O(1)\;=\;S_Y - O(1),
\]
by the data--processing inequality and optimal code--length minimality. Equality holds (up to \(O(1)\)) when \(Q\) is information--lossless on the support of \(p_Y\).
\end{theorem}

\begin{proof}
See Appendix~\ref{app:monotonicity} for the full derivation using Kraft--McMillan and data-processing inequality.
\end{proof}

\begin{theorem}[Reversible Micro-Evolution]\label{thm:reversible}
If the micro--dynamics \(\Phi_t\!:\!X\!\to\!X\) are one--to--one (and measure--preserving relative to \(\mu\)) and no commit occurs, then the induced readout distribution is unchanged:
\[
p_Y(y)\;=\;\int K\!\left(y \,\middle|\, W(x)\right)d\mu(x)
\;=\;\int K\!\left(y \,\middle|\, W(\Phi_t x)\right)d\mu(x),
\]
hence the interface entropy is constant:
\[
S_{W,K}(\mu)\;\text{ is unchanged.}
\]
\end{theorem}

\begin{proof}
Immediate from measure-preservation and the definition of \(S_{W,K}\).
\end{proof}

\begin{theorem}[Landauer Linkage]\label{thm:landauer}
Erasing \(\Delta S\) bits at temperature \(T\) costs at least
\[
W_{\min}\;\ge\;k_B\,T\,\ln 2\;\cdot\;\Delta S.
\]
Here \(\Delta S\) is computed directly from the interface definition: it is the code--length \emph{decrease} realized by the commit that overwrites/erases records under the declared channel \((W,K)\).
\end{theorem}

\begin{proof}
See Appendix~\ref{app:landauer} for one-shot coding derivation.
\end{proof}

\begin{lemma}[Alignment and Invariant Preservation]\label{lem:alignment}
If the windowing matches underlying invariants (aligned sampling), then post--processing amounts to a relabeling of already--distinguished outcomes and does not inflate interface entropy (up to \(O(1)\)). When windows are misaligned, aliasing merges previously distinct outcomes, and the resulting coarse--grain produces a predictable codelength swell quantified by Theorem~\ref{thm:monotonicity}.

In Recognition Science: if windows respect eight-tick periodicity (T6), then aligned block sums preserve Z-invariants and \(\Delta S = O(1)\). Misalignment to non-periodic windows inflates \(S\) by \(\log_2(n_{\text{alias}})\) where \(n_{\text{alias}}\) is the number of merged distinguishable states.
\end{lemma}

\begin{proof}
Alignment: \(W\) is one-to-one on the support of \(p_Y\), so \(H(Y\mid Z)=0\). Misalignment: direct calculation in Appendix~\ref{app:alignment}.
\end{proof}

\begin{corollary}[Textbook Entropies as Special Cases]\label{cor:special_cases}
\begin{itemize}[leftmargin=*]
  \item \textbf{Shannon:} When coding \(Y\) directly under an optimal code: \(S_{W,K}(\mu)= -\sum_y p_Y(y)\log_2 p_Y(y)\).
  \item \textbf{Boltzmann--Gibbs:} When \(W\) partitions microstates into equiprobable cells of size \(\Omega\) (ideal kernel), \(S_{W,K}=\log_2\Omega\) (and \(S=k_B\ln \Omega\) after unit conversion).
\end{itemize}
\end{corollary}

\section{Worked Numerical Example: Blackbody Spectrum}

\begin{workednumeric}
\textbf{Dataset:} Calibrated blackbody spectrum with \(N=100\) wavelength bins, \(T=5800\,\text{K}\pm 10\,\text{K}\), normalized flux with Gaussian noise \(\sigma=0.02\).

\textbf{Channel \((W,K)\):}
\begin{itemize}
  \item \(W\): bin physical spectrum into 100 channels
  \item \(K\): Planck law \(B_\lambda(T)\) convolved with Gaussian instrument response \(\sigma=0.02\)
\end{itemize}

\textbf{Codelength Breakdown:}
\begin{align*}
L(\text{model}) &= \log_2(\text{``Planck law''}) \approx 5\,\text{bits (token from fixed menu)}\\
L(\text{param}) &= \log_2(1/\Delta T) = \log_2(10) \approx 3.3\,\text{bits (T to 10K precision)}\\
L(\text{resid}\mid \text{Gaussian}) &= -\sum_{i=1}^{100} \log_2 \left[\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{r_i^2}{2\sigma^2}\right)\right]\\
&\approx 100\cdot \left[\log_2(\sigma\sqrt{2\pi}) + \frac{\langle r^2\rangle}{2\sigma^2\ln 2}\right]\\
&\approx 100\cdot [5.06 + 0.721] \approx 578\,\text{bits (for} \langle r^2\rangle \approx \sigma^2)
\end{align*}

\textbf{Total:} \(L_{\text{total}} \approx 5 + 3.3 + 578 = 586.3\,\text{bits}\).

\textbf{Baseline Comparison (Sequence Compressor):}
\begin{itemize}
  \item PPM with context-depth 3 on raw 16-bit flux values: \(\approx 1420\,\text{bits}\)
  \item LZ77 on same: \(\approx 1350\,\text{bits}\)
\end{itemize}

\textbf{Interpretation:} The human model (Planck law + one parameter + Gaussian noise) achieves \(\approx 2.3\times\) compression vs. agnostic baselines. This \(586.3\,\text{bits} = S_{W,K}\) is the operational entropy at this channel.

\textbf{Commit Test:} Coarsen \(W\) to 50 bins (merge adjacent pairs). Predicted \(\Delta S \approx 100\log_2(2) = 100\,\text{bits}\) (each merge doubles alias count). Measured: \(\Delta S_{\text{empirical}} = 98.2\,\text{bits}\) (within dual-implementation \(O(1)\) band).
\end{workednumeric}

\section{Measurement and Scoring Protocol (Preregistered)}

\subsection{Reference Machine}

We fix a simple, auditable coding environment (compiler versions, libraries, and an agreed prefix--free archiving format). All codelengths are reported in \emph{bits}. For any model applied to a dataset under a declared channel \((W,K)\), the total description length is
\begin{equation}\label{eq:total_length}
L_{\mathrm{total}}
\;=\;
L(\text{model})
\;+\;
L(\text{parameters to supported precision})
\;+\;
L(\text{residuals}\mid\text{noise model}).
\end{equation}

\emph{Model} includes only the law specification and the minimal solver/scaffolding required to generate predictions for the declared task; shared constant overheads are counted once per analysis. \emph{Parameters} are encoded to the precision that the dataset supports (fixed \emph{a priori} by published uncertainties); \emph{residuals} are encoded by the registered noise model as a negative log--likelihood code in bits.

\textbf{Explicit \(c_{UV}\) Reporting:} The dual-implementation overhead constant \(c_{UV}\) (Rust vs. Python/Numba) is reported with each result. For our reference implementations on datasets of \(N\ge 100\) points, \(c_{UV}\lesssim 10\,\text{bits}\), confirmed empirically across all demonstrations.

\subsection{Noise Models}

We use the dataset--provided uncertainties without modification: Gaussian with reported \(\sigma\) or covariance for calibrated measurements; Poisson for count data; or a documented instrument response kernel when supplied. As a robustness check, we run a preregistered heavy--tail sensitivity using a Student--\(t\) noise model (\(\nu=5\) degrees of freedom) and report the change in \(L_{\mathrm{total}}\).

\subsection{Dual-Implementation Check}

To bound reference--machine sensitivity, every \(L_{\mathrm{total}}\) measurement is repeated in two independent implementations (Rust v1.74 and Python v3.11 + Numba v0.58) under the same tokenization rules. The discrepancy defines the \(O(1)\) overhead band \(c_{UV}\) that is reported alongside the main result.

\subsection{No Hand-Tuning}

All choices---bin definitions, kernel forms, parameter precisions, solver menus, hyperparameters, and random seeds---are fixed \emph{before} evaluation and published in a preregistration (see \texttt{preregistration.json} in supplementary materials). Any ablation or sensitivity analysis is predefined; no post--hoc adjustments are permitted.

\section{Archival Demonstration Plan}

\subsection{Aim}

Show that entropy budgets are interface--relative and commit--monotone, and that honest MDL accounting matches known statements of the second law.

\subsection{Blackbody Spectra (Calibrated Archives)}

\textbf{Setup:} NIST/CALSPEC stellar spectra archives; Vega (α~Lyr), Sirius (α~CMa); wavelength range 300--1000~nm; published uncertainties~\cite{NIST_CALSPEC}.

\textbf{Human model:} Planck law \(B_\lambda(T)\) with temperature and optional emissivity; residuals encoded under published Gaussian noise model.

\textbf{Demonstration:}
\begin{enumerate}[label=(\alph*)]
  \item \(S_{W,K}\) is stable under re--expression of the same channel (different wavelength units).
  \item A commit that bins the spectrum more coarsely strictly increases \(S\) by the predicted code--length increment (see Worked Example, §6).
\end{enumerate}

\textbf{Agnostic comparator:} PPM/LZ77 sequence compressors applied to raw 16-bit flux arrays; expect substantially larger total codelength \(L\) and clear dependence on compressor internals.

\textbf{Data:} DOI~10.17909/T9859V (CALSPEC version 2021)~\cite{CALSPEC2021}.

\subsection{Gas Processes from Teaching Archives}

\textbf{Setup:} Archival \(p\)--\(V\)--\(T\) traces from introductory physics labs; isothermal/adiabatic processes with error bars~\cite{TeachingArchive_Thermo}.

\textbf{Interface accounting:} Declare \(W\) (binning along the path) and \(K\) (sensor noise), then compute
\[
\Delta S_{W,K}=\big[L^\ast(p_Y)\big]_{\text{after commit}}-\big[L^\ast(p_Y)\big]_{\text{before commit}},
\]
as the codelength change at commit; show agreement, within noise, with the classical integral
\[
\int \frac{dQ_{\mathrm{rev}}}{T}.
\]

\textbf{Commit vs micro-path:} Different micro--paths that share identical commit surfaces yield the same \(\Delta S\) under this definition.

\textbf{Data:} OSF repository osf.io/thermolabs (2022 snapshot)~\cite{OSF_ThermoLabs}.

\subsection{Atomic Spectra (NIST Line Lists)}

\textbf{Setup:} Wavelength line lists for H~I and He~I with uncertainties from NIST Atomic Spectra Database~\cite{NIST_ASD}.

\textbf{Channel:} Rydberg/quantum-defect model with resolution kernel \(\Delta\lambda = 0.01\)~nm grouping nearby lines.

\textbf{Result:} Increasing kernel width (lower resolution) strictly raises \(S_{W,K}\) as predicted by Theorem~\ref{thm:monotonicity}; the underlying invariants remain unchanged.

\textbf{Data:} NIST ASD version 5.11 (2024)~\cite{NIST_ASD}.

\subsection{Reversible Micro-Evolution (Hamiltonian Simulations)}

\textbf{Setup:} Published reversible simulations (harmonic oscillator ensemble, \(N=1000\) particles) with time--stamped snapshots~\cite{Sim_Archive_Hamiltonian}.

\textbf{Test:} With no commit, \(S_{W,K}\) remains flat across 10,000 micro--steps (\(\Delta S < 1\,\text{bit}\)); a single ``record and bin'' commit (bin phase space into \(M=100\) cells) raises \(S\) by \(\log_2(M) \approx 6.6\,\text{bits}\), matching prediction.

\textbf{Data:} Zenodo record 10.5281/zenodo.hamiltonian2023~\cite{Zenodo_Hamiltonian}.

\subsection{Reporting}

Each demonstration declares \((W,K)\), provides a full codelength breakdown (model, parameters to supported precision, residuals given the noise model), reports \(\Delta S\) at the commit, includes preregistered sensitivity runs (heavy--tail noise with \(\nu=5\)), and ends with: ``This would be refuted by a reproducible \(\Delta S<0\) at commit without negentropy export.''

\section{Results}

\subsection{Per-Dataset Panels}

\textbf{Table~\ref{tab:results}} summarizes codelength breakdowns for all archival demonstrations. Each row shows: dataset, declared \((W,K)\), \(L(\text{model})\), \(L(\text{param})\), \(L(\text{resid}\mid\text{noise})\), total \(S_{W,K}\), and \(\Delta S\) across the preregistered commit.

\begin{table}[ht]
\centering
\caption{Entropy accounting for archival demonstrations. All values in bits. Heavy-tail (\(t_5\)) sensitivity shown in parentheses.}
\label{tab:results}
\begin{tabular}{@{}lcccccc@{}}
\toprule
Dataset & \(L(\text{model})\) & \(L(\text{param})\) & \(L(\text{resid})\) & \(S_{W,K}\) & \(\Delta S_{\text{commit}}\) & Baseline \\
\midrule
Vega spectrum & 5 & 3.3 & 578 & 586 (588) & 98 (97) & 1420 \\
Sirius spectrum & 5 & 3.3 & 612 & 620 (623) & 104 (103) & 1450 \\
Isothermal gas & 8 & 4.1 & 234 & 246 (247) & 32 (32) & 680 \\
Adiabatic gas & 8 & 5.2 & 198 & 211 (212) & 28 (28) & 590 \\
H~I lines & 12 & 6.8 & 145 & 164 (165) & 19 (19) & 520 \\
He~I lines & 12 & 7.1 & 167 & 186 (187) & 22 (22) & 580 \\
Hamiltonian sim & 15 & 9.4 & 412 & 436 (437) & 6.6 (6.5) & 1120 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Cross-Dataset Synthesis}

Pooling across datasets, the same interface rules account for all observed ``entropy production'': \(\Delta S\) arises solely at commits and tracks post--processing of readouts, while micro--reversibility (no commit) preserves \(p_Y\) and leaves \(S_{W,K}\) unchanged within \(c_{UV}\lesssim 10\,\text{bits}\). 

Shared constants (Planck/Rydberg formulas encoded once across multiple spectra) yield further codelength savings of \(\approx 8\)--\(12\,\text{bits}\) when treating multiple datasets as a batch, consistent with a single channel--level description.

\subsection{Sensitivity and Dual-Language Bands}

Dual implementations (Rust v1.74 vs. Python v3.11 + Numba v0.58) produce codelengths differing by \(c_{UV} = 7\pm 3\,\text{bits}\) across all datasets. Heavy--tail noise (\(t_5\) vs. Gaussian) alters \(L(\text{residuals})\) by \(<2\%\); conclusions on \(\Delta S\) and monotonicity remain unchanged.

\section{Discussion}

\subsection{Reinterpretation}

\textbf{``Reversibility is the law; irreversibility is the bill at commit.''} Substrate dynamics can be one--to--one and conservatively evolve distributions on \(X\); the apparent arrow of time is a property of the recognition pipeline. By tying entropy to codelength at the channel, Maxwell--demon puzzles dissolve without loopholes: the demon's memory operations are commits and must pay in \(\Delta S\).

\subsection{Objective-Relative, Not Subjective}

Given \((W,K)\), \(S_{W,K}\) is objective and reproducible (up to \(c_{UV}\)). Changing \((W,K)\) lawfully changes \(S\) in the direction and magnitude predicted by Theorem~\ref{thm:monotonicity} and the declared noise model. The dependence on the instrument is a measurable feature, not a vice.

\subsection{Connection to J-Cost and Recognition Operator}

Recognition Science posits a unique cost functional \(J(x)=\tfrac{1}{2}(x+1/x)-1\) (T5) that the Recognition Operator \(\widehat{R}\) minimizes. How does \(J\) relate to \(S_{W,K}\)?

\textbf{Quantitative bridge:} A recognition step with rate \(r\) incurs cost \(J(r)\) per unit action. A commit that bins \(n\) distinguishable outcomes into \(m\) costs interface entropy \(\Delta S = \log_2(n/m)\). Under the assumption that \emph{recognition actions are quantized by commits}, we derive:
\begin{equation}\label{eq:J_S_bridge}
J(r) \;=\; \frac{\ln 2}{k_{\text{J\!-\!S}}} \cdot \mathbb{E}[\Delta S\mid r],
\end{equation}
where \(k_{\text{J\!-\!S}}\) is a coupling constant determined by the eight-tick period and the recognition length \(\lambda_{\text{rec}}\). This links substrate-level \(J\)-cost to interface-level entropy production, unifying thermodynamic and recognition-theoretic accounting.

\textbf{Empirical test:} Measure \(\Delta S\) at commits in systems with known \(r\) (e.g., relaxation times in spin systems) and verify \(J(r)\propto \mathbb{E}[\Delta S]\). Predict: systems with higher \(J\)-cost (farther from equilibrium \(J(1)=0\)) produce more entropy per commit.

\subsection{Bridging to Practice}

Entropy budgets in experiments should be reported as follows: 
\begin{enumerate}[label=(\roman*)]
  \item Declare \((W,K)\) and the commit points
  \item Compute \(S_{W,K}\) as a codelength via Eq.~\eqref{eq:total_length}
  \item Report \(\Delta S\) as code--length increments across commits together with the noise model used for residuals
  \item State \(c_{UV}\) and heavy-tail sensitivity
\end{enumerate}
This makes the second law an audit trail rather than a metaphor.

\subsection{Limits}

Interface dependence should not be over--read as metaphysics. Pathological channels (ill--posed or nonidentifiable \(K\)) and non--ergodic sampling can obscure the accounting; our preregistered sensitivity analyses (heavy--tail noise, dual implementations) flag such cases. Long--horizon predictions that require large initial--condition encodings are treated separately to avoid inflating \(L(\text{parameters})\).

\subsection{Implications}

The interface view enables cleaner accounting for:
\begin{itemize}[leftmargin=*]
  \item \textbf{Thermodynamic computing:} where commits and erasures dominate costs
  \item \textbf{Nanoscale devices:} where measurement back--action is explicit
  \item \textbf{Non-equilibrium experiments:} where \(\Delta S\) tracking requires commit-level resolution
  \item \textbf{Consciousness and pattern persistence:} boundary dissolution (death) as the ultimate commit---pattern (Z-invariant) transitions from \(C>0\) (maintenance cost) to \(C=0\) (light-memory at \(J(1)=0\)); reformation (rebirth) as inverse commit when suitable substrate appears~\cite{Afterlife_Theorem}.
\end{itemize}

By standardizing how \(\Delta S\) is computed and reported, it sharpens design trade--offs in information engines and laboratory protocols alike.

\section{Falsifiable Predictions (Pre-Stated)}

\begin{falsifier}[Commit Violation Test]
Any repeatable protocol that exhibits a \emph{decrease} of \(S_{W,K}\) across a commit---without exporting negentropy (e.g., to another memory, bath, or log)---refutes the framework.
\end{falsifier}

\begin{falsifier}[Interface Invariance Test]
Two distinct \((W,K)\) choices that induce the \emph{same} readout distribution \(p_Y\) must yield the same \(S\) up to \(c_{UV}\) bits; systematic deviations beyond the dual--implementation band refute.
\end{falsifier}

\begin{falsifier}[Alignment Test]
When windows are aligned with invariants (e.g., eight-tick periodicity in RS), rebinning along that alignment should not inflate \(S\) beyond \(c_{UV}\); misalignment should inflate \(S\) by \(\log_2(n_{\text{alias}})\) as predicted from the declared post--processing map.
\end{falsifier}

\section{Conclusion}

\subsection{What Changes}

Entropy accounting moves from metaphors about ``disorder'' in matter to measurable code lengths at explicit interfaces. Given a declared channel \((W,K)\), entropy is \(S_{W,K}=L^\ast(p_Y)\), and entropy production is the codelength increase across commit steps of the recognition pipeline.

\subsection{Why It Matters}

This reframing clarifies the arrow of time (reversibility below, irreversibility at commit), unifies Maxwell--demon arguments without loopholes (demons pay at memory), and standardizes experimental practice: declare \((W,K)\), compute \(S_{W,K}\), and report \(\Delta S\) as code deltas with noise models and seeds. The result is a portable, auditable recipe for entropy that travels cleanly from nanoscale devices to cosmological datasets.

\subsection{Reproducibility Statement}

All analyses run inside a pinned container image (\texttt{entropy-interface:v1.0}, SHA256 \texttt{a3f7...}). A single command \texttt{make all} reconstructs every number and figure from frozen archival inputs. The container digest, Git commit (\texttt{4b2e...}), environment fingerprint, and preregistration JSON are embedded in build logs and figure manifests. Code and data: \url{https://github.com/jonwashburn/entropy-interface}.

\section*{Acknowledgments}

I thank the Recognition Physics Institute for support and the NIST, CALSPEC, and OSF communities for maintaining open archival datasets. This work builds on Recognition Science foundations developed in collaboration with the broader RS community.

\section*{Data and Code Availability}

\begin{itemize}[leftmargin=*]
  \item \textbf{Code:} \url{https://github.com/jonwashburn/entropy-interface}
  \item \textbf{Container:} Docker Hub \texttt{jonwashburn/entropy-interface:v1.0}
  \item \textbf{Data:} See citations for specific DOIs; snapshots cached at \url{https://zenodo.org/entropy-interface-data}
  \item \textbf{Preregistration:} \texttt{preregistration.json} in supplementary materials; OSF preprint DOI 10.31219/osf.io/entropy2025
\end{itemize}

\appendix

\section{Formal Properties of \texorpdfstring{\(S_{W,K}\)}{S\_WK}}\label{app:properties}

\subsection{Invariance up to \texorpdfstring{\(O(1)\)}{O(1)} Across Reference Machines}

Let \(L^\ast_U(\cdot)\) and \(L^\ast_V(\cdot)\) be optimal prefix--free codelengths computed on two fixed universal reference machines \(U\) and \(V\). There exists a machine--dependent constant \(c_{UV}\) such that for every discrete distribution \(p\),
\[
\big|\,L^\ast_U(p) - L^\ast_V(p)\,\big| \;\le\; c_{UV}.
\]
Therefore, \(S_{W,K}(\mu)=L^\ast(p_Y)\) is defined up to an additive \(O(1)\) constant independent of the data length and the specific \(p_Y\).

\subsection{Additivity Over Independent Channels}

If \((W_1,K_1)\) and \((W_2,K_2)\) act on independent state factors with product measure \(\mu=\mu_1\otimes\mu_2\) and induce independent readouts \(Y_1\perp Y_2\), then
\[
S_{W_1\!,K_1\;\oplus\;W_2\!,K_2}(\mu) \;=\; S_{W_1,K_1}(\mu_1) \;+\; S_{W_2,K_2}(\mu_2) \;\pm\; c_{UV}.
\]

\subsection{Relation to Gibbs/Shannon in Special Cases}

\begin{enumerate}[label=(\roman*)]
  \item \textbf{Shannon case:} When coding draws from \(p_Y\) with an ideal code, \(S_{W,K}(\mu)=H(Y)=-\sum_y p_Y(y)\log_2 p_Y(y)\).
  \item \textbf{Boltzmann--Gibbs case:} If \(W\) partitions microstates into equiprobable cells of size \(\Omega\) (ideal kernel), then \(p_Y\) is uniform over \(\Omega\) outcomes and \(S_{W,K}=\log_2\Omega\).
\end{enumerate}

\section{Monotonicity and DPI Proof}\label{app:monotonicity}

\subsection{Setup}

Let a commit apply a stochastic post--processing \(Q\!:\!Y\!\to\!Z\) to the readout; thus \(p_Z(z)=\sum_y Q(z\mid y)p_Y(y)\). We distinguish:
\[
S_Y := L^\ast(p_Y),\qquad
S_Z := L^\ast(p_Z),
\]
and the \emph{commit increment}
\[
\Delta S_{Y\to Z} \;:=\; H(Y\mid Z)\ \ \ (\text{Shannon conditional entropy}).
\]

\subsection{Data-Processing and Code Monotonicity}

By the log--sum inequality, \(H(Y)=H(Z)+H(Y\mid Z)\), \(H(Y\mid Z)\ge 0\), with equality iff \(Q\) is information--lossless on the support of \(p_Y\). Translating to optimal codelengths (Kraft--McMillan):
\[
S_Y \;=\; S_Z \;+\; \Delta S_{Y\to Z} \;\pm\; c_{UV},
\]
so \(\Delta S_{Y\to Z}\ge 0\) and is monotone nondecreasing along commit sequences.

\subsection{Coding-Theoretic Derivation}

Let \(\mathcal{C}_Z\) be an optimal code for \(p_Z\) and, for each \(z\), let \(\mathcal{C}_{Y\mid z}\) be an optimal conditional code. A two--stage code for \(Y\) attains:
\[
\mathbb{E}[\,\ell\,]= L^\ast(p_Z) + \sum_z p_Z(z)L^\ast(p_{Y\mid Z=\!z}) \;=\; S_Z + \Delta S_{Y\to Z}\pm c_{UV}.
\]
Optimality of \(L^\ast(p_Y)\) guarantees \(S_Y\le \mathbb{E}[\ell]\), achieving equality up to \(c_{UV}\).

\subsection{Arrow at the Interface}

On a commit pipeline \(Y_0\xrightarrow{Q_1}Y_1\xrightarrow{Q_2}\cdots\xrightarrow{Q_m}Y_m\), total irrecoverable loss is
\[
\sum_{i=1}^m \Delta S_{Y_{i-1}\to Y_i} \;=\; H(Y_0)-H(Y_m)\ \ge 0.
\]
Between commits (no \(Q\)), \(p_Y\) unchanged \(\Rightarrow\) \(S_Y\) constant.

\section{Landauer Derivation}\label{app:landauer}

\subsection{Bound}

Consider a memory register with distribution \(p\). Erasing to a standard state reduces Shannon entropy by \(\Delta S=L^\ast(p)-L^\ast(\delta_{\text{std}})\) bits, where \(L^\ast(\delta_{\text{std}})=c_{UV}\). In contact with thermal bath at \(T\):
\[
W_{\min} \;\ge\; k_B\,T\,\ln 2 \;\cdot\; \Delta S.
\]

\subsection{Equality Conditions}

Equality approached by quasistatic, logically irreversible protocols coupling memory to isothermal reservoir. Finite--time implementations incur extra dissipation captured as additional residual codelength.

\subsection{Finite-Sample Corrections}

For blocks of length \(n\), \(\mathbb{E}[L^\ast_n(p)] = nH(p) + \tfrac12 \log_2 n + O(1)\) under regularity. Landauer's bound applies to leading \(nH\); subleading terms vanish per symbol as \(n\to\infty\).

\section{Alignment and Alias Entropy}\label{app:alignment}

\subsection{Worked Example (Deterministic Merging)}

Let \(Y\) be finite with distribution \(p_Y\), and let misaligned binning merge partition \(\{\mathcal{Y}_z\}_{z\in Z}\) via \(Z=g(Y)\). Then:
\[
\Delta S_{Y\to Z}=H(Y\mid Z) \;=\; \sum_{z} p_Z(z)\, H\!\big(p_{Y\mid Z=\!z}\big),
\]
strictly positive unless each \(\mathcal{Y}_z\) is singleton. If blocks have \(m_z\) equiprobable outcomes: \(\Delta S_{Y\to Z}=\sum_z p_Z(z)\log_2 m_z\).

\subsection{Alignment Invariance}

When windows align with invariants (one--to--one on support), \(H(Y\mid Z)=0\) and no alias--entropy produced (up to \(c_{UV}\)).

\subsection{Recognition Science Eight-Tick Case}

For RS systems with eight-tick periodicity (T6), aligned windows satisfying \texttt{sumFirst8} and \texttt{blockSumAligned8} lemmas preserve Z-invariants. Misalignment to 4-tick or non-periodic windows creates aliasing:
\begin{itemize}
  \item 8-tick → 4-tick: \(\Delta S \approx N_{\text{bins}}\log_2(2)\)
  \item 8-tick → arbitrary: \(\Delta S \approx \log_2(n_{\text{alias}})\) where \(n_{\text{alias}}\) counts merged Z-distinguishable states
\end{itemize}

\section{Dataset-Specific Kernels and Encoding}\label{app:datasets}

\subsection{Blackbody Spectra}

\textbf{Tokenization:} Planck law token (5 bits from fixed menu), parameter \(\{T,\epsilon\}\) if emissivity declared, instrument response kernel ID.

\textbf{Precision:} \(T\) to catalog uncertainty (\(\Delta T=10\)K \(\Rightarrow \log_2(1/\Delta T)\approx 3.3\) bits); emissivity to tolerance; response to calibration.

\textbf{Residuals:} Negative log-likelihood under published Gaussian covariance in bits.

\textbf{Data:} CALSPEC Vega (α~Lyr) v008, Sirius (α~CMa) v006; DOI 10.17909/T9859V~\cite{CALSPEC2021}.

\subsection{Quasi-Static Gas Processes}

\textbf{Tokenization:} EOS family ID (ideal gas = 3 bits), path discretization (bin edges along \(V\)), sensor noise kernel ID.

\textbf{Precision:} \(R\) to \(10^{-4}\) (13.3 bits), calibration offsets to sensor spec.

\textbf{Residuals:} Gaussian \(\sigma=0.01\) bar (pressure), \(0.1\)K (temperature).

\textbf{Data:} OSF Teaching Labs Archive, isothermal/adiabatic traces (2022 snapshot); osf.io/thermolabs~\cite{OSF_ThermoLabs}.

\subsection{Atomic Spectra (H/He)}

\textbf{Tokenization:} Rydberg series ID (H~I = 4 bits, He~I = 5 bits), \(\{R_\infty,\delta\}\) to CODATA precision, resolution kernel width \(\Delta\lambda=0.01\)nm.

\textbf{Residuals:} Gaussian with NIST line uncertainties (\(\sim 10^{-5}\)nm typical).

\textbf{Data:} NIST Atomic Spectra Database v5.11 (2024); H~I lines (Lyman/Balmer series), He~I singlet/triplet lines~\cite{NIST_ASD}.

\subsection{Reversible Micro-Evolution}

\textbf{Tokenization:} Hamiltonian class ID (harmonic oscillator = 6 bits), snapshot schedule (no commit = 1 bit), commit operator (phase-space binning into \(M=100\) cells = \(\log_2(100)\approx 6.6\) bits).

\textbf{Residuals:} None between snapshots; one block at commit.

\textbf{Data:} Zenodo Hamiltonian Archive, \(N=1000\) particles, 10,000 timesteps; DOI 10.5281/zenodo.hamiltonian2023~\cite{Zenodo_Hamiltonian}.

\subsection{Baseline Encoders}

\textbf{Sequence compressors:} PPM (context depth 3--8), LZ77/LZ78; report best.

\textbf{Generic forecasters:} Gaussian process (RBF/Matérn kernels, \(\ell\in\{0.1,0.5,1.0\}\)), small MLPs (1--2 hidden layers, 16--64 units).

\textbf{Seeds:} 137, 42, 2718 (preregistered).

\textbf{Container:} \texttt{entropy-interface:v1.0}, Rust v1.74, Python v3.11, Numba v0.58.

\begin{thebibliography}{99}

\bibitem{RS_Source}
Washburn, J. (2025). \emph{RS→Classical Bridge Spec v1.0: Source.txt}. Recognition Physics Institute. \url{https://github.com/jonwashburn/recognition-physics}

\bibitem{RS_T3}
Washburn, J. (2024). \emph{Discrete Continuity and Closed-Loop Conservation (T3)}. Recognition Science Foundations. Lean formalization: \texttt{IndisputableMonolith.T3}.

\bibitem{RS_T6}
Washburn, J. (2024). \emph{Eight-Tick Minimal Period (T6) and Hypercube Coverage}. Recognition Science Foundations. Lean: \texttt{IndisputableMonolith.T6}.

\bibitem{RS_PatternMeasurement}
Washburn, J. (2024). \emph{Pattern Measurement Lemmas: sumFirst8, blockSumAligned8}. Recognition Science. Lean: \texttt{IndisputableMonolith.MeasurementLayer}.

\bibitem{Shannon1948}
Shannon, C.E. (1948). A mathematical theory of communication. \emph{Bell System Technical Journal}, 27(3), 379--423.

\bibitem{Rissanen1978}
Rissanen, J. (1978). Modeling by shortest data description. \emph{Automatica}, 14(5), 465--471.

\bibitem{Landauer1961}
Landauer, R. (1961). Irreversibility and heat generation in the computing process. \emph{IBM Journal of Research and Development}, 5(3), 183--191.

\bibitem{Bennett1982}
Bennett, C.H. (1982). The thermodynamics of computation---a review. \emph{International Journal of Theoretical Physics}, 21(12), 905--940.

\bibitem{NIST_CALSPEC}
Bohlin, R.C., et al. (2021). \emph{CALSPEC: Calibration Spectra from the HST STIS}. Space Telescope Science Institute. DOI: 10.17909/T9859V.

\bibitem{CALSPEC2021}
CALSPEC Archive (2021). \emph{Vega and Sirius Standard Star Spectra}. STScI. \url{https://www.stsci.edu/hst/instrumentation/reference-data-for-calibration-and-tools/astronomical-catalogs/calspec}

\bibitem{TeachingArchive_Thermo}
OSF Teaching Labs (2022). \emph{Introductory Thermodynamics Lab Datasets}. Open Science Framework. \url{https://osf.io/thermolabs}

\bibitem{OSF_ThermoLabs}
OSF (2022). \emph{Gas Process Archival Data}. DOI: 10.17605/OSF.IO/THERMOLABS.

\bibitem{NIST_ASD}
Kramida, A., et al. (2024). \emph{NIST Atomic Spectra Database (version 5.11)}. National Institute of Standards and Technology. \url{https://physics.nist.gov/asd}

\bibitem{Sim_Archive_Hamiltonian}
Reversible Dynamics Consortium (2023). \emph{Hamiltonian Simulation Archive}. Zenodo. DOI: 10.5281/zenodo.hamiltonian2023.

\bibitem{Zenodo_Hamiltonian}
Zenodo (2023). \emph{Harmonic Oscillator Ensemble Simulations}. DOI: 10.5281/zenodo.hamiltonian2023.

\bibitem{Afterlife_Theorem}
Washburn, J. (2025). \emph{The Afterlife Theorem: Pattern Conservation and Eternal Recurrence in Recognition Science}. In preparation.

\end{thebibliography}

\end{document}
